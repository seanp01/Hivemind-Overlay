// webpack handles this with .env
const LMPORT = process.env.LMPORT || 5222;

const sentiment_to_label = {
    // ðŸŸ¢ Tone-based sentiments
    "neutral": 0,              // ðŸ˜ No strong sentiment
    "positive": 1,             // ðŸ˜Š Kind, optimistic, supportive
    "negative": 2,             // ðŸ˜  Disapproving, pessimistic
    "toxic": 3,                // â˜ ï¸ Aggressive, rude, inflammatory
    "confused": 4,             // ðŸ˜• Expresses confusion or lack of understanding
    "angry": 5,                // ðŸ˜¡ Expresses frustration or anger
    "sad": 6,                  // ðŸ˜¢ Expresses disappointment, loss, or empathy
    "hype": 7,                 // ðŸ”¥ Excited cheering or support (e.g. "LETS GOOO")
    "agreeable": 8,            // ðŸ‘ Signals agreement, like "yep", "true", "based"
    "supportive": 9,           // ðŸ¤— Deeply affirming, emotionally positive
    "playful": 10,             // ðŸ˜œ Silly, teasing, or lighthearted tone
    "reaction": 11,            // ðŸ˜² General expressive response to events

    // ðŸŽ­ Expression style / delivery
    "sarcasm": 12,             // ðŸ˜ Ironic, saying the opposite of what's meant
    "humor": 13,               // ðŸ˜‚ Light-hearted humor, not mocking
    "copypasta": 14,           // ðŸ“‹ Repeated or meme block text
    "emote_spam": 15,          // ðŸ¤ª Emote-only or excessive emotes
    "bait": 16,                // ðŸŽ£ Provocative to stir a reaction
    "mocking": 17,             // ðŸ˜ Ridiculing someone/something
    "cringe": 18,              // ðŸ˜¬ Social embarrassment, second-hand shame

    // â“ Intent or purpose of message
    "question": 19,            // â“ Seeking info, asking streamer or chat
    "joke": 20,                // ðŸ˜‚ Joke or kidding tone

    "command_request": 21,     // â— Suggesting actions ("play X", "go here")
    "insightful": 22,          // ðŸ’¡ Adds valuable knowledge or perspective
    "meta": 23,                // ðŸ§  Commentary about chat or the stream itself
    "criticism": 24,           // ðŸ§ Disapproval or critique, non-toxic

    // ðŸ§© Add-on specialized classes
    "backseat": 25,            // ðŸª‘ Telling the streamer how to play
    "fan_theory": 26,          // ðŸ§© Lore speculation or plot guessing
    "personal_story": 27,      // ðŸ“– Sharing personal anecdotes to relate

    // ðŸ§  Fine-grained interaction labels
    "commentary": 28,          // ðŸ—£ï¸ Observational, running commentary
    "affirmative": 29,         // âœ… Confirming message ("true", "yep", etc)
    "compliment": 30,          // ðŸ¥° Direct praise or flattery

    // Additional mappings
    "mixed": 31,               // ðŸ¤” Mixed sentiment
    "happy": 32,               // ðŸ˜„ Happy
    "surprised": 33,           // ðŸ˜² Surprised
    "fear": 34,                // ðŸ˜± Fear
    "conversation": 35,        // ðŸ’¬ Conversation
    "default": 36              // ðŸ’¬ Default/unspecified
};


class LLMService {
    constructor(apiUrl) {
        // Use provided apiUrl or default to localhost with LMPORT
        this.apiUrl = apiUrl || new URL(`/predict`, `http://localhost:${LMPORT}`).toString();
    }

    async summarizeChat(chatBlocks) {
        // Step 1: Get a summary for each chat block
        const summaries = await Promise.all(
            chatBlocks.map(block => this.callPredict(block))
        );

        // Step 2: Optionally, combine summaries and get a final summary
        const combinedSummary = summaries.join('\n');
        const finalSummary = await this.callPredict(
            `Summarize the following chat summaries:\n${combinedSummary}`
        );

        return finalSummary;
    }

    createPrompt(chatBlocks) {
        return `Summarize the following chat messages:\n${chatBlocks.join('\n')}`;
    }

    // Send a prompt to the local LLM server's /predict endpoint
    async callPredict(chatMessages) {
        try {
            const response = await fetch(this.apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ chat_messages: chatMessages })
            });

            if (!response.ok) {
                // Optionally log the error response
                // const errorText = await response.text();
                // console.error('LLM API error:', errorText);
                return [{ sentiment: 'error', score: 0, message: 'Failed to fetch prediction from LLM API' }];
            }

            const data = await response.json();
            // Reverse the sentiment_to_label mapping for label lookup
            const labelToSentiment = Object.entries(sentiment_to_label)
                .reduce((acc, [sentiment, idx]) => {
                    acc[`LABEL_${idx}`] = sentiment;
                    return acc;
                }, {});

            // Map predictions to readable sentiment labels
            return (data.prediction || []).map(pred => ({
                sentiment: labelToSentiment[pred.label] || pred.label,
                score: pred.score
            }));
        } catch (error) {
            // Handle network or parsing errors gracefully
            // Optionally log the error
            // console.error('Fetch failed:', error);
            return [{ sentiment: 'error', score: 0, message: error.message || 'Unknown error' }];
        }
    }
}

export default LLMService;